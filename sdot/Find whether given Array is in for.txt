#multiple regression

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics
import warnings
warnings.filterwarnings('ignore')
insurance_dataset=pd.read_csv('D:\VIT\sem 6\ml\lab 1\insurance.csvâ€™)
insurance_dataset.head(10)

insurance_dataset.info()

insurance_dataset.isnull().sum()

#distribution of the age
sns.set()
plt.figure(figsize=(6,6))
sns.distplot(insurance_dataset['age'])
plt.title('Age Distribution')
plt.show()

#gender column
plt.figure(figsize=(6,6))
sns.countplot(x='sex',data=insurance_dataset)
plt.title('Sex Distribution')
plt.show()

#Value count based on the gender
insurance_dataset['sex'].value_counts()

#BMI Distribution
sns.set()
plt.figure(figsize=(6,6))
sns.distplot(insurance_dataset['bmi'])
plt.title('Age Distribution')
plt.show()

# children column
plt.figure(figsize=(6,6))
sns.countplot(x='children',data=insurance_dataset)
plt.title('Children')
plt.show()

insurance_dataset['children'].value_counts()

# smoker column
plt.figure(figsize=(6,6))
sns.countplot(x='smoker',data=insurance_dataset)
plt.title('Smoker')
plt.show()

#value count based on smoker or not 
insurance_dataset['smoker'].value_counts()

#region column
plt.figure(figsize=(6,6))
sns.countplot(x='region',data=insurance_dataset)
plt.title('Region')
plt.show()

#value count base on the region 
insurance_dataset['region'].value_counts()

#distribution of 'charges' value
sns.set()
plt.figure(figsize=(6,6))
sns.distplot(insurance_dataset['charges'])
plt.title('Charge Distribution')
plt.show()

# DATA PREPROCESSING
# Encoding the categorical data
#encoding 'sex' column
insurance_dataset.replace({'sex':{'male':0,'female':1}},inplace=True)
#encoding 'smoker' column
insurance_dataset.replace({'smoker':{'yes':0,'no':1}},inplace=True)
#encoding 'region' column
insurance_dataset.replace({'region':{'southeast':0,'southwest':1,'northeast':2,'northwest':3}},inplace=True)
print(insurance_dataset)

X=insurance_dataset.drop(columns='charges',axis=1)
Y=insurance_dataset['charges']
print(X)

print(Y)

#Splitting the training and testing data
X_train,X_test,Y_train,Y_test=train_test_split(X, Y, test_size=0.2, 
random_state=2)
print(X_test)

print(X.shape,X_train.shape,X_test.shape)

print(Y.shape,Y_train.shape,Y_test.shape)

# TRAINING THE MODEL
#Loading the Linear Regression Model
regressor=LinearRegression()
regressor.fit(X_train,Y_train)
LinearRegression()
#Model Evaluation
#prediction on training data
training_data_prediction=regressor.predict(X_train)
# R squared value for training data
r2_train=metrics.r2_score(Y_train,training_data_prediction)
print("R-squared value",r2_train)
R-squared value 0.751505643411174
#prediction on testing data
testing_data_prediction=regressor.predict(X_test)
# R squared value for testing data
r2_test=metrics.r2_score(Y_test,testing_data_prediction)
print("R-squared value",r2_test)
R-squared value 0.7447273869684076
#Building a predictive system
input_data=(31,1,25.74,0,1,0)
#changing input_data to a numpy array
input_data_array=np.asarray(input_data)
print(input_data_array)
#Reshaping the array
input_data_reshaped=input_data_array.reshape(1,-1)
print(input_data_reshaped)
prediction=regressor.predict(input_data_reshaped)
print(prediction)
print("The insurance cost is USD",prediction[0])



#liner regression

%matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (20.0, 10.0)
data = pd.read_csv("headbrain.csv") #Download the dataset and give the
appropriate path
print(data.shape)
data.head()

X=data['Head Size(cm^3)'].values
Y=data['Brain Weight(grams)'].values
mean_x=np.mean(X)
mean_y=np.mean(Y)
n=len(X) #Total number of values
numer=0
denom=0
for i in range(n):
 numer+=(X[i]-mean_x) * (Y[i] - mean_y)
 denom+=(X[i]-mean_x) ** 2
b1=numer/denom
b0=mean_y - (b1*mean_x)
print(b1,b0) #Print cooefficients
0.26342933948939945 325.57342104944223
max_x=np.max(X)+100
min_x=np.min(X)-100
x=np.linspace(min_x, max_x, 1000)
y=b0+b1*x
plt.plot(x,y,color='#58b970', label='Regression Line') #Ploting line
plt.scatter(X, Y, c='#ef5423', label='Scatter Plot') #Ploting Scatter 
Points
plt.xlabel('Head Size in cm3')
plt.ylabel('Brain Weight in grams')
plt.legend()
plt.show()




#decison tree and PCA

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import numpy as np
import matplotlib.pyplot as plt

from sklearn import decomposition
from sklearn import datasets


# Loading the dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Train, test splits
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, 
 stratify=y, random_state=42)

# Decision trees with depth = 2
clf = DecisionTreeClassifier(max_depth=2, random_state=42)
clf.fit(X_train, y_train)
preds = clf.predict_proba(X_test)
print('Accuracy: {:.5f}'.format(accuracy_score(y_test,preds.argmax(axis=1))))


pca = decomposition.PCA(n_components=2)
X_centered = X - X.mean(axis=0)
pca.fit(X_centered)
X_pca = pca.transform(X_centered)


# Test-train split and apply PCA
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=.3,stratify=y,random_state=42)

clf = DecisionTreeClassifier(max_depth=2, random_state=42)
clf.fit(X_train, y_train)
preds = clf.predict_proba(X_test)
print('Accuracy: {:.5f}'.format(accuracy_score(y_test, preds.argmax(axis=1))))



#liner regression

#Linear-Regression
%matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (20.0, 10.0)
data = pd.read_csv("Advertising Budget and Sales.csv") #Download the 
dataset and give the appropriate path
print(data.shape)
data.head()
(200, 5)
 Unnamed: 0 TV Ad Budget ($) Radio Ad Budget ($) Newspaper Ad 
Budget ($) \
0 1 230.1 37.8 
69.2 
1 2 44.5 39.3 
45.1 
2 3 17.2 45.9 
69.3 
3 4 151.5 41.3 
58.5 
4 5 180.8 10.8 
58.4 
 Sales ($) 
0 22.1 
1 10.4 
2 9.3 
3 18.5 
4 12.9 
data.columns.tolist()
['Unnamed: 0',
 'TV Ad Budget ($)',
 'Radio Ad Budget ($)',
 'Newspaper Ad Budget ($)',
 'Sales ($)']
import seaborn as sns
sns.pairplot(data=data, x_vars=['TV Ad Budget ($)', 'Radio Ad Budget 
($)', 'Newspaper Ad Budget ($)'], y_vars=['Sales ($)'], kind='reg', 
plot_kws={'marker':'o'})

X_simple = data['TV Ad Budget ($)']
y_simple = data['Sales ($)']
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
x_train,x_test,y_train,y_test=train_test_split(X_simple,y_simple,test_
size=0.3,random_state=30)
regression.fit(x_train.values.reshape(-1,1),y_train)
LinearRegression()
regression=LinearRegression()
y_pred=regression.predict(x_test.values.reshape(-1,1))
#mse=mean_squared_error(y_test,y_pred)
#print(f'MSE of Simple Linear Regression: {mse}')
r2=r2_score(y_test,y_pred)
print(f'the accuracy of this model is {r2}')
the accuracy of this model is 0.6186011328120249
from sklearn.metrics import r2_score, mean_squared_error
#Multi-linear Regression
x_col = list(range(3))
y_col = list(range(3, 4))
X = data.iloc[:, x_col]
y = data.iloc[:, y_col]

X_train, X_test, y_train, y_test = train_test_split(X, y, 
test_size=0.167, random_state=42)
multi_linear = LinearRegression()
multi_linear.fit(X_train, y_train)
LinearRegression()
y_preds = multi_linear.predict(X_test)
mse = mean_squared_error(y_test, y_preds)
print(f'MSE of Multi-Linear Regression: {mse}')
r2 = r2_score(y_test, y_preds)
print(f'R2 Score of Multi-Linear Regression: {r2}')
MSE of Multi-Linear Regression: 685.0799582981989
R2 Score of Multi-Linear Regression: 0.07623150339310192
features = list(data.drop(['Sales ($)'], axis=1).columns)
target = y_train.columns.tolist()
for feats in features:
 sns.regplot(data, x=data[feats], y=data[target], 
scatter_kws={'alpha': 0.3}, label=feats)
plt.legend()
plt.xlabel('Independent Variables')
plt.ylabel('Dependent Variable')
plt.title('Multi-Linear Regression Graph')
plt.show()



#random forest

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import warnings
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
warnings.filterwarnings('ignore')

 df= pd.read_csv('C:/Users/student/Downloads/Position_Salaries.csv')
print(df)

df.info()

 # Assuming df is your DataFrame
X = df.iloc[:,1:2].values#features
y = df.iloc[:,2].values# Target variable

 label_encoder = LabelEncoder()
x_categorical = df.select_dtypes(include=['object']).apply(label_encoder.fit_transform
x_numerical = df.select_dtypes(exclude=['object']).values
x = pd.concat([pd.DataFrame(x_numerical), x_categorical], axis=1).values
# Fitting Random Forest Regression to the dataset
regressor = RandomForestRegressor(n_estimators=10, random_state=0, oob_score=True)
# Fit the regressor with x and y data
regressor.fit(x, y)

# Access the OOB Score
oob_score = regressor.oob_score_
print(f'Out-of-Bag Score: {oob_score}')
# Making predictions on the same data or new data
predictions = regressor.predict(x)
# Evaluating the model
mse = mean_squared_error(y, predictions)
print(f'Mean Squared Error: {mse}')
r2 = r2_score(y, predictions)
print(f'R-squared: {r2}')

 X_grid = np.arange(min(X),max(X),0.01)
X_grid = X_grid.reshape(len(X_grid),1)
plt.scatter(X,y, color='blue') #plotting real points
plt.plot(X_grid, regressor.predict(X_grid),color='green') #plotting for predict points
plt.title("Random Forest Regression Results")
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
# Assuming regressor is your trained Random Forest model
# Pick one tree from the forest, e.g., the first tree (index 0)
tree_to_plot = regressor.estimators_[0]
# Plot the decision tree
plt.figure(figsize=(20, 10))
plot_tree(tree_to_plot, feature_names=df.columns.tolist(), filled=True, rounded=True,
plt.title("Decision Tree from Random Forest")
plt.show()



# k means

# import the necessary libraries 
import json 
import numpy as np 
import pandas as pd 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.decomposition import PCA 
from sklearn.cluster import KMeans 
import matplotlib.pyplot as plt 

path="C:/Users/student/Downloads/archive/Sarcasm_Headlines_Dataset.json"
print(path)
df=pd.read_json(path, lines=True)

# Extract the sentence only 
sentence = df.headline 

# create vectorizer 
vectorizer = TfidfVectorizer(stop_words='english') 

# vectorizer the text documents 
vectorized_documents = vectorizer.fit_transform(sentence)

# Convert to dense array for easier manipulation
dense_documents = vectorized_documents.toarray()

# Check for infs and NaNs and replace them with a suitable value, e.g., 0
dense_documents = np.where(np.isnan(dense_documents), 0, dense_documents)
dense_documents = np.where(np.isinf(dense_documents), 0, dense_documents)



#random forest

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import warnings
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
warnings.filterwarnings('ignore')

df= pd.read_csv('C:/Users/student/Downloads/Position_Salaries.csv')
print(df)

df.info()

# Assuming df is your DataFrame
X = df.iloc[:,1:2].values#features
y = df.iloc[:,2].values# Target variable

label_encoder = LabelEncoder()
x_categorical = df.select_dtypes(include=['object']).apply(label_encoder.fit_transform
x_numerical = df.select_dtypes(exclude=['object']).values
x = pd.concat([pd.DataFrame(x_numerical), x_categorical], axis=1).values

# Fitting Random Forest Regression to the dataset
regressor = RandomForestRegressor(n_estimators=10, random_state=0, oob_score=True)

# Fit the regressor with x and y data
regressor.fit(x, y)

 # Access the OOB Score
oob_score = regressor.oob_score_
print(f'Out-of-Bag Score: {oob_score}')
# Making predictions on the same data or new data
predictions = regressor.predict(x)
# Evaluating the model
mse = mean_squared_error(y, predictions)
print(f'Mean Squared Error: {mse}')
r2 = r2_score(y, predictions)
print(f'R-squared: {r2}')

X_grid = np.arange(min(X),max(X),0.01)
X_grid = X_grid.reshape(len(X_grid),1)
plt.scatter(X,y, color='blue') #plotting real points
plt.plot(X_grid, regressor.predict(X_grid),color='green') #plotting for predict points
plt.title("Random Forest Regression Results")
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
# Assuming regressor is your trained Random Forest model
# Pick one tree from the forest, e.g., the first tree (index 0)
tree_to_plot = regressor.estimators_[0]
# Plot the decision tree
plt.figure(figsize=(20, 10))
plot_tree(tree_to_plot, feature_names=df.columns.tolist(), filled=True, rounded=True,
plt.title("Decision Tree from Random Forest")
plt.show()



#liner SVM

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix
from matplotlib.colors import ListedColormap
import pandas as pd

dataset=pd.read_csv("C:/Users/student/Downloads/data.csv")

X, y = datasets.make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, random_state=42)

# Extracting Independent and Dependent variables
X = X  # Independent variables
y = y  # Dependent variable

# Splitting the dataset into training and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting the SVM classifier to the training set
linear_svm = SVC(kernel='linear')
linear_svm.fit(X_train, y_train)

# Predicting the test set results
linear_predictions = linear_svm.predict(X_test)

# Calculate accuracy
linear_accuracy = accuracy_score(y_test, linear_predictions)

# Creating the confusion matrix
cm = confusion_matrix(y_test, linear_predictions)

# Visualizing the training set results
def visualize_results(X_set, y_set, title):
    X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                         np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
    plt.contourf(X1, X2, linear_svm.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
                 alpha = 0.75, cmap = ListedColormap(('red', 'green')))
    plt.xlim(X1.min(), X1.max())
    plt.ylim(X2.min(), X2.max())
    for i, j in enumerate(np.unique(y_set)):
        plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                    c = ListedColormap(('red', 'green'))(i), label = j)
    plt.title(title)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.show()

visualize_results(X_train, y_train, 'Linear SVM (Training Set)')




#mnist

import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

mnist = fetch_openml('mnist_784', version=1, cache=True)
X = mnist["data"]
y = mnist["target"].astype(np.uint8)

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Scaling the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.astype(np.float32))
X_test_scaled = scaler.transform(X_test.astype(np.float32))

from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score

# Training a Linear SVC model
lin_clf = LinearSVC(random_state=42)
lin_clf.fit(X_train_scaled, y_train)

# Predicting and evaluating the model
y_pred = lin_clf.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
print(f"Linear SVC Accuracy: {accuracy}")

from sklearn.svm import SVC

svm_clf = SVC(kernel='rbf', gamma='scale', random_state=42)
svm_clf.fit(X_train_scaled, y_train)

# Predicting and evaluating the model
y_pred = svm_clf.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
print(f"SVC with RBF Kernel Accuracy: {accuracy}")

# Training a SVC model with Polynomial kernel
svm_poly_clf = SVC(kernel='poly', degree=3, gamma='scale', random_state=42)
svm_poly_clf.fit(X_train_scaled, y_train)

# Predicting and evaluating the model
y_pred = svm_poly_clf.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
print(f"SVC with Polynomial Kernel Accuracy: {accuracy}")


from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import reciprocal, uniform

# Defining the parameter distributions
param_distributions = {"gamma": reciprocal(0.001, 0.1), "C": uniform(1, 10)}

# Setting up the randomized search
rnd_search_cv = RandomizedSearchCV(SVC(kernel='rbf', gamma='scale'), param_distributions, n_iter=100, cv=5, random_state=42)
rnd_search_cv.fit(X_train_scaled, y_train)

# Evaluating the best model
best_svm_clf = rnd_search_cv.best_estimator_
y_pred = best_svm_clf.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
print(f"Best SVC with RBF Kernel Accuracy: {accuracy}")



#non linear svm